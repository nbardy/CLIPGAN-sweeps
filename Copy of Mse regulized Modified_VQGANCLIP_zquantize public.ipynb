{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Copy of Mse regulized Modified_VQGANCLIP_zquantize public.ipynb","provenance":[{"file_id":"1gFn9u3oPOgsNzJWEFmdK-N9h_y65b8fj","timestamp":1627246811023}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"CppIQlPhhwhs"},"source":["# Generates images from text prompts with VQGAN and CLIP (z+quantize method).\n","\n","By jbustter https://twitter.com/jbusted1 .\n","Based on a notebook by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings)\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkUfzT60ZZ9q","executionInfo":{"status":"ok","timestamp":1627246626037,"user_tz":420,"elapsed":275,"user":{"displayName":"Nicholas Bardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2u5A7yXc9PX1orgEtAm_IgffrkH-4Pnm-MkpPRNI=s64","userId":"11124157157678434342"}},"outputId":"4fcd5e05-0df5-4c3b-b758-bb852512bdba"},"source":["!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Sun Jul 25 20:57:05 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"wSfISAhyPmyp","executionInfo":{"status":"ok","timestamp":1627246654021,"user_tz":420,"elapsed":27989,"user":{"displayName":"Nicholas Bardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2u5A7yXc9PX1orgEtAm_IgffrkH-4Pnm-MkpPRNI=s64","userId":"11124157157678434342"}},"outputId":"9ff17586-1c74-4e67-dd53-7ebc16036b8e"},"source":["!git clone https://github.com/openai/CLIP\n","!git clone https://github.com/CompVis/taming-transformers\n","!pip install ftfy regex tqdm omegaconf pytorch-lightning\n","!pip install kornia\n","!pip install einops"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'CLIP'...\n","remote: Enumerating objects: 115, done.\u001b[K\n","remote: Counting objects: 100% (24/24), done.\u001b[K\n","remote: Compressing objects: 100% (14/14), done.\u001b[K\n","remote: Total 115 (delta 10), reused 16 (delta 9), pack-reused 91\u001b[K\n","Receiving objects: 100% (115/115), 6.25 MiB | 29.10 MiB/s, done.\n","Resolving deltas: 100% (50/50), done.\n","Cloning into 'taming-transformers'...\n","remote: Enumerating objects: 756, done.\u001b[K\n","remote: Total 756 (delta 0), reused 0 (delta 0), pack-reused 756\u001b[K\n","Receiving objects: 100% (756/756), 202.21 MiB | 39.07 MiB/s, done.\n","Resolving deltas: 100% (188/188), done.\n","Collecting ftfy\n","  Downloading ftfy-6.0.3.tar.gz (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n","Collecting omegaconf\n","  Downloading omegaconf-2.1.0-py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 2.6 MB/s \n","\u001b[?25hCollecting pytorch-lightning\n","  Downloading pytorch_lightning-1.3.8-py3-none-any.whl (813 kB)\n","\u001b[K     |████████████████████████████████| 813 kB 48.2 MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n","Collecting PyYAML>=5.1.*\n","  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n","\u001b[K     |████████████████████████████████| 636 kB 44.0 MB/s \n","\u001b[?25hCollecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 65.5 MB/s \n","\u001b[?25hRequirement already satisfied: pillow!=8.3.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (7.1.2)\n","Collecting future>=0.17.1\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 48.3 MB/s \n","\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n","\u001b[K     |████████████████████████████████| 118 kB 57.6 MB/s \n","\u001b[?25hCollecting tensorboard!=2.5.0,>=2.2.0\n","  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n","\u001b[K     |████████████████████████████████| 10.6 MB 59.0 MB/s \n","\u001b[?25hCollecting torchmetrics>=0.2.0\n","  Downloading torchmetrics-0.4.1-py3-none-any.whl (234 kB)\n","\u001b[K     |████████████████████████████████| 234 kB 67.7 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.9.0+cu102)\n","Collecting pyDeprecate==0.3.0\n","  Downloading pyDeprecate-0.3.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (21.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.23.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 46.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning) (2.4.7)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.17.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.32.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.34.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (57.2.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.36.2)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.6.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n","\u001b[K     |████████████████████████████████| 142 kB 52.7 MB/s \n","\u001b[?25hCollecting yarl<2.0,>=1.0\n","  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 60.2 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n","Collecting async-timeout<4.0,>=3.0\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.5.0)\n","Building wheels for collected packages: ftfy, antlr4-python3-runtime, future\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41934 sha256=355efdfbd4815dfc16e20499caab2579bf452bb99ca3a95a04f87d266f7c22e0\n","  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=f91943dd666f16d476936f6fe395c0d78dca13cb147a7947c4914b2ebdd7c8ec\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=2853910d866b3fb4b5a9e690fc2459bc9419d2593b7398788cc239c437478fe0\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","Successfully built ftfy antlr4-python3-runtime future\n","Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, torchmetrics, tensorboard, PyYAML, pyDeprecate, future, antlr4-python3-runtime, pytorch-lightning, omegaconf, ftfy\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.5.0 requires tensorboard~=2.5, but you have tensorboard 2.4.1 which is incompatible.\u001b[0m\n","Successfully installed PyYAML-5.4.1 aiohttp-3.7.4.post0 antlr4-python3-runtime-4.8 async-timeout-3.0.1 fsspec-2021.7.0 ftfy-6.0.3 future-0.18.2 multidict-5.1.0 omegaconf-2.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.8 tensorboard-2.4.1 torchmetrics-0.4.1 yarl-1.6.3\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting kornia\n","  Downloading kornia-0.5.6-py2.py3-none-any.whl (301 kB)\n","\u001b[?25l\r\u001b[K     |█                               | 10 kB 24.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 40 kB 39.5 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 51 kB 39.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 61 kB 41.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 71 kB 30.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 81 kB 27.8 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 92 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 102 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 112 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 122 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 133 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 143 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 153 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 163 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 174 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 184 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 194 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 204 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 215 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 225 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 235 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 245 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 256 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 266 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 276 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 286 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 296 kB 27.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 301 kB 27.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from kornia) (1.9.0+cu102)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->kornia) (3.7.4.3)\n","Installing collected packages: kornia\n","Successfully installed kornia-0.5.6\n","Collecting einops\n","  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n","Installing collected packages: einops\n","Successfully installed einops-0.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhhdWrSxQhwg","executionInfo":{"status":"ok","timestamp":1627246717220,"user_tz":420,"elapsed":63207,"user":{"displayName":"Nicholas Bardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2u5A7yXc9PX1orgEtAm_IgffrkH-4Pnm-MkpPRNI=s64","userId":"11124157157678434342"}},"outputId":"395a238c-4948-4d47-ca22-1e5388850c6b"},"source":["\n","!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_16384.yaml\n","!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_16384.ckpt\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100   692  100   692    0     0   1704      0 --:--:-- --:--:-- --:--:--  1704\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n","100  934M  100  934M    0     0  14.9M      0  0:01:02  0:01:02 --:--:-- 14.9M\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EXMSuW2EQWsd","executionInfo":{"status":"ok","timestamp":1627246722456,"user_tz":420,"elapsed":5238,"user":{"displayName":"Nicholas Bardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2u5A7yXc9PX1orgEtAm_IgffrkH-4Pnm-MkpPRNI=s64","userId":"11124157157678434342"}}},"source":["import argparse\n","import math\n","from pathlib import Path\n","import sys\n","\n","sys.path.append('./taming-transformers')\n","\n","from IPython import display\n","from omegaconf import OmegaConf\n","from PIL import Image\n","from taming.models import cond_transformer, vqgan\n","import torch\n","from torch import nn, optim\n","from torch.nn import functional as F\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from tqdm.notebook import tqdm\n","import numpy as np\n","\n","from CLIP import clip\n","\n","import kornia.augmentation as K"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"JvnTBhPGT1gn","executionInfo":{"status":"ok","timestamp":1627246723604,"user_tz":420,"elapsed":1149,"user":{"displayName":"Nicholas Bardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2u5A7yXc9PX1orgEtAm_IgffrkH-4Pnm-MkpPRNI=s64","userId":"11124157157678434342"}}},"source":["def noise_gen(shape):\n","    n, c, h, w = shape\n","    noise = torch.zeros([n, c, 1, 1])\n","    for i in reversed(range(5)):\n","        h_cur, w_cur = h // 2**i, w // 2**i\n","        noise = F.interpolate(noise, (h_cur, w_cur), mode='bicubic', align_corners=False)\n","        noise += torch.randn([n, c, h_cur, w_cur]) / 5\n","    return noise\n","\n","\n","def sinc(x):\n","    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n","\n","\n","def lanczos(x, a):\n","    cond = torch.logical_and(-a < x, x < a)\n","    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n","    return out / out.sum()\n","\n","\n","def ramp(ratio, width):\n","    n = math.ceil(width / ratio + 1)\n","    out = torch.empty([n])\n","    cur = 0\n","    for i in range(out.shape[0]):\n","        out[i] = cur\n","        cur += ratio\n","    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n","\n","\n","def resample(input, size, align_corners=True):\n","    n, c, h, w = input.shape\n","    dh, dw = size\n","\n","    input = input.view([n * c, 1, h, w])\n","\n","    if dh < h:\n","        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n","        pad_h = (kernel_h.shape[0] - 1) // 2\n","        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n","        input = F.conv2d(input, kernel_h[None, None, :, None])\n","\n","    if dw < w:\n","        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n","        pad_w = (kernel_w.shape[0] - 1) // 2\n","        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n","        input = F.conv2d(input, kernel_w[None, None, None, :])\n","\n","    input = input.view([n, c, h, w])\n","    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n","    \n","\n","# def replace_grad(fake, real):\n","#     return fake.detach() - real.detach() + real\n","\n","\n","class ReplaceGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x_forward, x_backward):\n","        ctx.shape = x_backward.shape\n","        return x_forward\n","\n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        return None, grad_in.sum_to_size(ctx.shape)\n","\n","\n","class ClampWithGrad(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, input, min, max):\n","        ctx.min = min\n","        ctx.max = max\n","        ctx.save_for_backward(input)\n","        return input.clamp(min, max)\n","\n","    @staticmethod\n","    def backward(ctx, grad_in):\n","        input, = ctx.saved_tensors\n","        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n","\n","replace_grad = ReplaceGrad.apply\n","\n","clamp_with_grad = ClampWithGrad.apply\n","# clamp_with_grad = torch.clamp\n","\n","def vector_quantize(x, codebook):\n","    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n","    indices = d.argmin(-1)\n","    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n","    return replace_grad(x_q, x)\n","\n","\n","class Prompt(nn.Module):\n","    def __init__(self, embed, weight=1., stop=float('-inf')):\n","        super().__init__()\n","        self.register_buffer('embed', embed)\n","        self.register_buffer('weight', torch.as_tensor(weight))\n","        self.register_buffer('stop', torch.as_tensor(stop))\n","\n","    def forward(self, input):\n","        \n","        input_normed = F.normalize(input.unsqueeze(1), dim=2)#(input / input.norm(dim=-1, keepdim=True)).unsqueeze(1)# \n","        embed_normed = F.normalize((self.embed).unsqueeze(0), dim=2)#(self.embed / self.embed.norm(dim=-1, keepdim=True)).unsqueeze(0)#\n","\n","        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n","        dists = dists * self.weight.sign()\n","        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n","\n","\n","def parse_prompt(prompt):\n","    vals = prompt.rsplit(':', 2)\n","    vals = vals + ['', '1', '-inf'][len(vals):]\n","    return vals[0], float(vals[1]), float(vals[2])\n","\n","def one_sided_clip_loss(input, target, labels=None, logit_scale=100):\n","    input_normed = F.normalize(input, dim=-1)\n","    target_normed = F.normalize(target, dim=-1)\n","    logits = input_normed @ target_normed.T * logit_scale\n","    if labels is None:\n","        labels = torch.arange(len(input), device=logits.device)\n","    return F.cross_entropy(logits, labels)\n","\n","class MakeCutouts(nn.Module):\n","    def __init__(self, cut_size, cutn, cut_pow=1.):\n","        super().__init__()\n","        self.cut_size = cut_size\n","        self.cutn = cutn\n","        self.cut_pow = cut_pow\n","\n","        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n","        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n","\n","    def set_cut_pow(self, cut_pow):\n","      self.cut_pow = cut_pow\n","\n","    def forward(self, input):\n","        sideY, sideX = input.shape[2:4]\n","        max_size = min(sideX, sideY)\n","        min_size = min(sideX, sideY, self.cut_size)\n","        cutouts = []\n","        cutouts_full = []\n","        \n","        min_size_width = min(sideX, sideY)\n","        lower_bound = float(self.cut_size/min_size_width)\n","        \n","        for ii in range(self.cutn):\n","            \n","            \n","          # size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n","          size = int(min_size_width*torch.zeros(1,).normal_(mean=.8, std=.3).clip(lower_bound, 1.)) # replace .5 with a result for 224 the default large size is .95\n","          # size = int(min_size_width*torch.zeros(1,).normal_(mean=.9, std=.3).clip(lower_bound, .95)) # replace .5 with a result for 224 the default large size is .95\n","\n","          offsetx = torch.randint(0, sideX - size + 1, ())\n","          offsety = torch.randint(0, sideY - size + 1, ())\n","          cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n","          cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n","\n","        \n","        cutouts = torch.cat(cutouts, dim=0)\n","\n","        # if args.use_augs:\n","        #   cutouts = augs(cutouts)\n","\n","        # if args.noise_fac:\n","        #   facs = cutouts.new_empty([cutouts.shape[0], 1, 1, 1]).uniform_(0, args.noise_fac)\n","        #   cutouts = cutouts + facs * torch.randn_like(cutouts)\n","        \n","\n","        return clamp_with_grad(cutouts, 0, 1)\n","\n","\n","def load_vqgan_model(config_path, checkpoint_path):\n","    config = OmegaConf.load(config_path)\n","    if config.model.target == 'taming.models.vqgan.VQModel':\n","        model = vqgan.VQModel(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n","        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n","        parent_model.eval().requires_grad_(False)\n","        parent_model.init_from_ckpt(checkpoint_path)\n","        model = parent_model.first_stage_model\n","    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n","        model = vqgan.GumbelVQ(**config.model.params)\n","        model.eval().requires_grad_(False)\n","        model.init_from_ckpt(checkpoint_path)\n","    else:\n","        raise ValueError(f'unknown model type: {config.model.target}')\n","    del model.loss\n","    return model\n","\n","def resize_image(image, out_size):\n","    ratio = image.size[0] / image.size[1]\n","    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n","    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n","    return image.resize(size, Image.LANCZOS)\n","\n","class TVLoss(nn.Module):\n","    def forward(self, input):\n","        input = F.pad(input, (0, 1, 0, 1), 'replicate')\n","        x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n","        y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n","        diff = x_diff**2 + y_diff**2 + 1e-8\n","        return diff.mean(dim=1).sqrt().mean()\n","\n","class GaussianBlur2d(nn.Module):\n","    def __init__(self, sigma, window=0, mode='reflect', value=0):\n","        super().__init__()\n","        self.mode = mode\n","        self.value = value\n","        if not window:\n","            window = max(math.ceil((sigma * 6 + 1) / 2) * 2 - 1, 3)\n","        if sigma:\n","            kernel = torch.exp(-(torch.arange(window) - window // 2)**2 / 2 / sigma**2)\n","            kernel /= kernel.sum()\n","        else:\n","            kernel = torch.ones([1])\n","        self.register_buffer('kernel', kernel)\n","\n","    def forward(self, input):\n","        n, c, h, w = input.shape\n","        input = input.view([n * c, 1, h, w])\n","        start_pad = (self.kernel.shape[0] - 1) // 2\n","        end_pad = self.kernel.shape[0] // 2\n","        input = F.pad(input, (start_pad, end_pad, start_pad, end_pad), self.mode, self.value)\n","        input = F.conv2d(input, self.kernel[None, None, None, :])\n","        input = F.conv2d(input, self.kernel[None, None, :, None])\n","        return input.view([n, c, h, w])\n","\n","class EMATensor(nn.Module):\n","    \"\"\"implmeneted by Katherine Crowson\"\"\"\n","    def __init__(self, tensor, decay):\n","        super().__init__()\n","        self.tensor = nn.Parameter(tensor)\n","        self.register_buffer('biased', torch.zeros_like(tensor))\n","        self.register_buffer('average', torch.zeros_like(tensor))\n","        self.decay = decay\n","        self.register_buffer('accum', torch.tensor(1.))\n","        self.update()\n","    \n","    @torch.no_grad()\n","    def update(self):\n","        if not self.training:\n","            raise RuntimeError('update() should only be called during training')\n","\n","        self.accum *= self.decay\n","        self.biased.mul_(self.decay)\n","        self.biased.add_((1 - self.decay) * self.tensor)\n","        self.average.copy_(self.biased)\n","        self.average.div_(1 - self.accum)\n","\n","    def forward(self):\n","        if self.training:\n","            return self.tensor\n","        return self.average\n","\n","%mkdir /content/vids"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WN4OtaLbHBN6"},"source":["# ARGS"]},{"cell_type":"code","metadata":{"id":"tLw9p5Rzacso","executionInfo":{"status":"error","timestamp":1627246723607,"user_tz":420,"elapsed":9,"user":{"displayName":"Nicholas Bardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2u5A7yXc9PX1orgEtAm_IgffrkH-4Pnm-MkpPRNI=s64","userId":"11124157157678434342"}},"outputId":"750746ae-24c8-43e2-ddca-6174fda1f169","colab":{"base_uri":"https://localhost:8080/","height":129}},"source":["args = argparse.Namespace(\n","    \n","    prompts=[\"Halls of Space(Giant Mushroom Brain in a Dark Radiation Filled Corner of Space)  by Ben Wanat #scifi\n","\"],\n","    size=[640, 512], \n","    init_image= None,\n","    init_weight= 0.5,\n","\n","    # clip model settings\n","    clip_model='ViT-B/32',\n","    vqgan_config='vqgan_imagenet_f16_16384.yaml',         \n","    vqgan_checkpoint='vqgan_imagenet_f16_16384.ckpt',\n","    step_size=0.1,\n","    \n","    # cutouts / crops\n","    cutn=64,\n","    cut_pow=1,\n","\n","    # display\n","    display_freq=25,\n","    seed=158758,\n","    use_augs = True,\n","    noise_fac= 0.1,\n","    ema_val = 0.99,\n","\n","    record_generation=True,\n","\n","    # noise and other constraints\n","    use_noise = None,\n","    constraint_regions = False,#\n","    \n","    \n","    # add noise to embedding\n","    noise_prompt_weights = None,\n","    noise_prompt_seeds = [14575],#\n","\n","    # mse settings\n","    mse_withzeros = True,\n","    mse_decay_rate = 50,\n","    mse_epoches = 5,\n","    mse_quantize = True,\n","\n","    # end itteration\n","    max_itter = -1,\n",")\n","\n","mse_decay = 0\n","if args.init_weight:\n","  mse_decay = args.init_weight / args.mse_epoches\n","\n","# <AUGMENTATIONS>\n","augs = nn.Sequential(\n","    \n","    K.RandomHorizontalFlip(p=0.5),\n","    K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'), # padding_mode=2\n","    K.RandomPerspective(0.2,p=0.4, ),\n","    K.ColorJitter(hue=0.01, saturation=0.01, p=0.7),\n","\n","    )\n","\n","noise = noise_gen([1, 3, args.size[0], args.size[1]])\n","image = TF.to_pil_image(noise.div(5).add(0.5).clamp(0, 1)[0])\n","image.save('init3.png')"],"execution_count":6,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-a9f9eed6c39b>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    prompts=[\"Halls of Space(Giant Mushroom Brain in a Dark Radiation Filled Corner of Space)  by Ben Wanat #scifi\u001b[0m\n\u001b[0m                                                                                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"]}]},{"cell_type":"markdown","metadata":{"id":"crRQdV3jPSvw"},"source":["# Constraints"]},{"cell_type":"code","metadata":{"id":"OD9R97ygRN-0","executionInfo":{"status":"aborted","timestamp":1627246723605,"user_tz":420,"elapsed":6,"user":{"displayName":"Nicholas Bardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2u5A7yXc9PX1orgEtAm_IgffrkH-4Pnm-MkpPRNI=s64","userId":"11124157157678434342"}}},"source":["from PIL import Image, ImageDraw\n","\n","if args.constraint_regions and args.init_image:\n","  \n","  device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","  toksX, toksY = args.size[0] // 16, args.size[1] // 16\n","\n","  pil_image = Image.open(args.init_image).convert('RGB')\n","  pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n","\n","  width, height = pil_image.size\n","\n","  d = ImageDraw.Draw(pil_image)\n","  for i in range(0,width,16):\n","      d.text((i+4,0), f\"{int(i/16)}\", fill=(50,200,100))\n","  for i in range(0,height,16):\n","      d.text((4,i), f\"{int(i/16)}\", fill=(50,200,100))\n","\n","  pil_image = TF.to_tensor(pil_image)\n","\n","  print(pil_image.shape)\n","  for i in range(pil_image.shape[1]):\n","    for j in range(pil_image.shape[2]):\n","      if i%16 == 0 or j%16 ==0:\n","        pil_image[:,i,j] = 0\n","\n","  # select region\n","  c_h = [16,32]\n","  c_w = [0,40]\n","\n","  c_hf = [i*16 for i in c_h]\n","  c_wf = [i*16 for i in c_w]\n","\n","  pil_image[0,c_hf[0]:c_hf[1],c_wf[0]:c_wf[1]] = 0\n","\n","  TF.to_pil_image(pil_image.cpu()).save('progress.png')\n","  display.display(display.Image('progress.png'))\n","\n","  z_mask = torch.zeros([1, 256, int(height/16), int(width/16)]).to(device)\n","  z_mask[:,:,c_h[0]:c_h[1],c_w[0]:c_w[1]] = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QXgTa_JWi7Sn"},"source":["### Actually do the run..."]},{"cell_type":"code","metadata":{"id":"g7EDme5RYCrt","executionInfo":{"status":"aborted","timestamp":1627246723605,"user_tz":420,"elapsed":6,"user":{"displayName":"Nicholas Bardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2u5A7yXc9PX1orgEtAm_IgffrkH-4Pnm-MkpPRNI=s64","userId":"11124157157678434342"}}},"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","print('Using device:', device)\n","print('using prompts: ', args.prompts)\n","\n","tv_loss = TVLoss() \n","\n","model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n","perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n","mse_weight = args.init_weight\n","\n","cut_size = perceptor.visual.input_resolution\n","# e_dim = model.quantize.e_dim\n","\n","if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n","    e_dim = 256\n","    n_toks = model.quantize.n_embed\n","    z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n","    z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n","else:\n","    e_dim = model.quantize.e_dim\n","    n_toks = model.quantize.n_e\n","    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n","    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n","\n","\n","make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n","\n","f = 2**(model.decoder.num_resolutions - 1)\n","toksX, toksY = args.size[0] // f, args.size[1] // f\n","\n","if args.seed is not None:\n","    torch.manual_seed(args.seed)\n","\n","if args.init_image:\n","    pil_image = Image.open(args.init_image).convert('RGB')\n","    pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n","    pil_image = TF.to_tensor(pil_image)\n","    if args.use_noise:\n","      pil_image = pil_image + args.use_noise * torch.randn_like(pil_image) \n","    z, *_ = model.encode(pil_image.to(device).unsqueeze(0) * 2 - 1)\n","\n","else:\n","    \n","    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n","\n","    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n","        z = one_hot @ model.quantize.embed.weight\n","    else:\n","        z = one_hot @ model.quantize.embedding.weight\n","    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n","\n","\n","z = EMATensor(z, args.ema_val)\n","\n","if args.mse_withzeros and not args.init_image:\n","  z_orig = torch.zeros_like(z.tensor)\n","else:\n","  z_orig = z.tensor.clone()\n","\n","\n","opt = optim.Adam(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n","\n","normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n","                                 std=[0.26862954, 0.26130258, 0.27577711])\n","\n","pMs = []\n","\n","if args.noise_prompt_weights and args.noise_prompt_seeds:\n","  for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n","    gen = torch.Generator().manual_seed(seed)\n","    embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n","    pMs.append(Prompt(embed, weight).to(device))\n","\n","for prompt in args.prompts:\n","    txt, weight, stop = parse_prompt(prompt)\n","    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n","    pMs.append(Prompt(embed, weight, stop).to(device))\n","    # pMs[0].embed = pMs[0].embed + Prompt(embed, weight, stop).embed.to(device)\n","\n","\n","def synth(z, quantize=True):\n","    if args.constraint_regions:\n","      z = replace_grad(z, z * z_mask)\n","\n","    if quantize:\n","      if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n","      else:\n","        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n","\n","    else:\n","      z_q = z.model\n","\n","    return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n","\n","@torch.no_grad()\n","def checkin(i, losses):\n","    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n","    tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n","    out = synth(z.average, True)\n","\n","    TF.to_pil_image(out[0].cpu()).save('progress.png')   \n","    display.display(display.Image('progress.png')) \n","\n","\n","def ascend_txt():\n","    global mse_weight\n","\n","    out = synth(z.tensor)\n","\n","    if args.record_generation:\n","      with torch.no_grad():\n","        global vid_index\n","        out_a = synth(z.average, True)\n","        TF.to_pil_image(out_a[0].cpu()).save(f'/content/vids/{vid_index}.png')\n","        vid_index += 1\n","\n","    cutouts = make_cutouts(out)\n","\n","    if args.use_augs:\n","      cutouts = augs(cutouts)\n","\n","    if args.noise_fac:\n","      facs = cutouts.new_empty([args.cutn, 1, 1, 1]).uniform_(0, args.noise_fac)\n","      cutouts = cutouts + facs * torch.randn_like(cutouts)\n","\n","    iii = perceptor.encode_image(normalize(cutouts)).float()\n","\n","    result = []\n","\n","    if args.init_weight:\n","        \n","        global z_orig\n","        \n","        result.append(F.mse_loss(z.tensor, z_orig) * mse_weight / 2)\n","        # result.append(F.mse_loss(z, z_orig) * ((1/torch.tensor((i)*2 + 1))*mse_weight) / 2)\n","\n","        with torch.no_grad():\n","          if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate*args.mse_epoches:\n","\n","            if args.mse_quantize:\n","              z_orig = vector_quantize(z.average.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)#z.average\n","            else:\n","              z_orig = z.average.clone()\n","\n","            if mse_weight - mse_decay > 0 and mse_weight - mse_decay >= mse_decay:\n","              mse_weight = mse_weight - mse_decay\n","              print(f\"updated mse weight: {mse_weight}\")\n","            else:\n","              mse_weight = 0\n","              print(f\"updated mse weight: {mse_weight}\")\n","\n","    for prompt in pMs:\n","        result.append(prompt(iii))\n","\n","    return result\n","\n","vid_index = 0\n","def train(i):\n","    \n","    opt.zero_grad()\n","    lossAll = ascend_txt()\n","\n","    if i % args.display_freq == 0:\n","        checkin(i, lossAll)\n","    \n","    loss = sum(lossAll)\n","\n","    loss.backward()\n","    opt.step()\n","    z.update()\n","\n","i = 0\n","try:\n","    with tqdm() as pbar:\n","        while True and i != args.max_itter:\n","\n","            train(i)\n","\n","            if i > 0 and i%args.mse_decay_rate==0 and i <= args.mse_decay_rate * args.mse_epoches:\n","              z = EMATensor(z.average, args.ema_val)\n","              opt = optim.Adam(z.parameters(), lr=args.step_size, weight_decay=0.00000000)\n","\n","            i += 1\n","            pbar.update()\n","\n","except KeyboardInterrupt:\n","    pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CDUaCaRnUKMZ"},"source":["# create video"]},{"cell_type":"code","metadata":{"id":"DT3hKb5gJUPq","executionInfo":{"status":"aborted","timestamp":1627246723606,"user_tz":420,"elapsed":7,"user":{"displayName":"Nicholas Bardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2u5A7yXc9PX1orgEtAm_IgffrkH-4Pnm-MkpPRNI=s64","userId":"11124157157678434342"}}},"source":["%cd vids\n","\n","images = \"%d.png\"\n","video = \"/content/old_man_iceberg.mp4\"\n","!ffmpeg -r 30 -i $images -crf 20 -s 640x512 -pix_fmt yuv420p $video\n","\n","%cd .."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UiZMW3kAUD1f"},"source":["delete all frames from folder"]},{"cell_type":"code","metadata":{"id":"PsixT6gqJ8aY","executionInfo":{"status":"aborted","timestamp":1627246723606,"user_tz":420,"elapsed":6,"user":{"displayName":"Nicholas Bardy","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg2u5A7yXc9PX1orgEtAm_IgffrkH-4Pnm-MkpPRNI=s64","userId":"11124157157678434342"}}},"source":["%cd vids\n","%rm *.png\n","%cd .."],"execution_count":null,"outputs":[]}]}